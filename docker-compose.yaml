services:
  cim-fastapi:
    pull_policy: always
    image: goncaloferreirauva/cim-fastapi
    working_dir: /app
    ports:
      - "8000:8000"
    volumes:
      - ./.env:/app/.env:ro
      - ./allowed_emails.txt:/app/allowed_emails.txt
      # - ./_auth_server/users.db:/app/users.db # For debugging only.
      # - ./_publisher/publisher.py:/app/publisher/publisher.py:ro
      - ./_auth_server/login_server.py:/app/login_server.py:ro
    command: >
      sh -lc '
        test -f /app/requirements.txt || { echo "requirements.txt missing"; exit 1; }
        pip install --no-cache-dir -r /app/requirements.txt &&
        exec uvicorn login_server:app --host 0.0.0.0 --port 8000 --proxy-headers
      '
    environment:
      - MONGO_URI=mongodb://metrics-db:27017,metrics-db-2:27017,metrics-db-3:27017/?replicaSet=rs0
      
  metrics-db:
    image: mongo:7
    restart: unless-stopped
    volumes:
      - /~/data/metricsdb/metrics_data:/data/db # For SZTAKI, the HDD has the necessary space.
      - ./.cert:/certs:ro
    ports:
      - "27017:27017"
    secrets:
      - mongo_keyfile
    command: [
      "mongod", "--replSet", "rs0", "--bind_ip_all",
      # "--keyFile=/run/secrets/mongo_keyfile",
      # "--tlsMode=preferTLS",
      # "--tlsCertificateKeyFile=/certs/mongo.pem",
      # "--tlsCAFile=/certs/ca-bundle.pem"
    ]
    healthcheck:
      test: ["CMD", "mongosh", "--quiet", "--eval", "db.runCommand({ ping: 1 }).ok"]
      interval: 10s
      timeout: 5s
      retries: 10

  metrics-db-2:
    image: mongo:7
    restart: unless-stopped
    command: ["mongod", "--replSet", "rs0", "--bind_ip_all"]
    volumes:
      # - ./metricsdb/metrics_data_2:/data/db
      - /~/data/metricsdb/metrics_data_2:/data/db # For SZTAKI, the HDD has the necessary space.
    healthcheck:
      test: ["CMD-SHELL", "mongosh --quiet --eval \"db.adminCommand('ping').ok\" || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10

  metrics-db-3:
    image: mongo:7
    restart: unless-stopped
    command: ["mongod", "--replSet", "rs0", "--bind_ip_all"]
    volumes:
      # - ./metricsdb/metrics_data_3:/data/db
      - /~/data/metricsdb/metrics_data_3:/data/db # For SZTAKI, the HDD has the necessary space.
    healthcheck:
      test: ["CMD-SHELL", "mongosh --quiet --eval \"db.adminCommand('ping').ok\" || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10

    # Replica set initialiser (runs once then exits)
  mongo-rs-init:
    image: mongo:7
    depends_on:
      metrics-db:   { condition: service_healthy }
      metrics-db-2: { condition: service_healthy }
      metrics-db-3: { condition: service_healthy }
    entrypoint:
      - bash
      - -lc
      - |
        set -euo pipefail
        hosts=(metrics-db metrics-db-2 metrics-db-3)
        echo "Waiting for all Mongo nodes..."
        for h in "$${hosts[@]}"; do
          echo "  checking $$h:27017"
          ready=0
          for i in {1..60}; do
            if mongosh --host "$$h:27017" --quiet --eval "db.adminCommand({ ping: 1 }).ok" >/dev/null 2>&1; then
              echo "    $$h is up"
              ready=1
              break
            fi
            sleep 2
          done
          if [ $$ready -ne 1 ]; then
            echo "ERROR: $$h did not respond to ping within 120s" >&2
            exit 1
          fi
        done
        echo "Initiating replica set if needed..."
        mongosh --host metrics-db:27017 --quiet <<'JS'
        try {
          if (!rs.status().ok) { throw new Error("not initiated"); }
          print("Replica set already OK");
        } catch (e) {
          rs.initiate({
            _id: "rs0",
            members: [
              { _id: 0, host: "metrics-db:27017" },
              { _id: 1, host: "metrics-db-2:27017" },
              { _id: 2, host: "metrics-db-3:27017" }
            ]
          });
          var ok = false; for (var i=0;i<60;i++){ 
            try { if (rs.status().ok) { ok = true; break; } } catch(e){}
            sleep(1000);
          }
          if (!ok) { throw new Error("replica set did not become OK in time"); }
          print("Replica set initiated");
        }
        JS
    restart: "no"
  

  kpi-service:
    image: goncaloferreirauva/gd-wp6-kpi-service:latest
    pull_policy: always
    restart: unless-stopped
    env_file:
      - .env
    environment:
      - CI_PROVIDER=wattnet
      - WATTNET_BASE=https://api.wattnet.eu
      - RETAIN_DB_NAME=ci-retainment-db
      - RETAIN_COLL=pending_ci
      - RETAIN_TTL_SECONDS=172800
      - AUTH_VERIFY_URL=https://greendigit-cim.sztaki.hu/gd-cim-api/v1/verify-token
      - GD_BEARER_TOKEN=${JWT_TOKEN}
      - GOCDB_CERT=/etc/gocdb-cert/GDIGIT_Cert.pem # This has to be provided in the environment.
      - GOCDB_KEY=/etc/gocdb-cert/gd_gocdb_private.pem # This has to be provided in the environment.
    command: uvicorn main:app --host 0.0.0.0 --port 8011 --log-level info --reload --reload-dir /app
    ports:
      - "8011:8011"
    volumes:
      - "./sites_data/sites_latlngpue.json:/data/sites_latlngpue.json:ro"
      - "./.cert:/etc/gocdb-cert:ro"
      - "./_kpi/main.py:/app/main.py:ro" # Only for debugging
      

  # # Simple publisher that streams Mongo inserts to the webhook
  # mongo-stream-publisher:
  #   image: python:3.12-slim
  #   depends_on:
  #     kpi-service:  { condition: service_started }
  #     cim-fastapi: { condition: service_started }
  #     cim-service: { condition: service_started }
  #   env_file:
  #     - .env
  #   environment:
  #     - MONGO_URI=mongodb://metrics-db:27017,metrics-db-2:27017,metrics-db-3:27017/?replicaSet=rs0
  #     - WATCH_DB=metricsdb
  #     - WATCH_COLL=metrics
  #     - GD_BEARER_TOKEN=${JWT_TOKEN}
  #     - KPI_INTERNAL_ENDPOINT=http://kpi-service:8011/transform-and-forward
  #     - CIM_INTERNAL_ENDPOINT=http://cim-service:8012/transform-and-forward
  #     # - SITES_URL=http://kpi-service:8011/load-sites
  #   working_dir: /app
  #   # This is a mounted volume to have a real-time feedback while debugging. For the moment we do not need it.
  #   volumes:
  #     - ./_publisher/publisher.py:/app/publisher.py:ro
  #   command: bash -lc "pip install --no-cache-dir pymongo requests python-dateutil && exec python -u ./publisher.py"
  #   restart: unless-stopped

  # # Automated smoke-test to prove failover & integrity
  # rs-smoke-test:
  #   image: mongo:7
  #   # depends_on:
  #     # mongo-indexes: { condition: service_completed_successfully }
  #     # mongo-rs-init: { condition: service_completed_successfully }
  #   entrypoint:
  #     - bash
  #     - -lc
  #     - |
  #       set -e
  #       URI="mongodb://metrics-db:27017,metrics-db-2:27017,metrics-db-3:27017/?replicaSet=rs0"
  #       echo "Inserting a doc, stepping down primary, then reading back..."
  #       mongosh "$$URI" --quiet <<'JS'
  #       const dbm = db.getSiblingDB("smoketest");
  #       dbm.dropDatabase();
  #       dbm.t.insertOne({ _t: new Date(), note: "before stepdown" });
  #       let isMaster = db.isMaster ? db.isMaster() : db.hello();
  #       print("Primary is:", isMaster.primary);
  #       try { rs.stepDown(10); } catch(e) { print("stepDown:", e.codeName || e.message); }
  #       // wait for a new primary
  #       let ok=false; for (let i=0;i<60;i++){ 
  #         try { if ((db.isMaster?db.isMaster():db.hello()).ismaster){ ok=true; break; } } catch(e) {}
  #         sleep(1000);
  #       }
  #       if (!ok) throw new Error("no primary elected in time");
  #       dbm.t.insertOne({ _t: new Date(), note: "after stepdown" });
  #       const c = dbm.t.countDocuments({});
  #       print("Smoke test PASS. Documents:", c);
  #       JS
  #   restart: "no"

  # ci-retain-db-1:
  #   image: mongo:7
  #   command: ["mongod","--replSet","rs0","--bind_ip_all"]
  #   volumes: [ "./ci_retain_data_1:/data/db" ]
  #   restart: unless-stopped

  # ci-retain-db-2:
  #   image: mongo:7
  #   command: ["mongod","--replSet","rs0","--bind_ip_all"]
  #   volumes: [ "./ci_retain_data_2:/data/db" ]
  #   restart: unless-stopped

  # ci-retain-db-3:
  #   image: mongo:7
  #   command: ["mongod","--replSet","rs0","--bind_ip_all"]
  #   volumes: [ "./ci_retain_data_3:/data/db" ]
  #   restart: unless-stopped

  # ci-retain-rs-init:
  #   image: mongo:7
  #   depends_on:
  #     ci-retain-db-1: { condition: service_started }
  #     ci-retain-db-2: { condition: service_started }
  #     ci-retain-db-3: { condition: service_started }
  #   command: >
  #     bash -lc 'mongosh --host ci-retain-db-1:27017 --eval
  #     "rs.initiate({_id:\\"rs0\\",members:[
  #        {_id:0,host:\\"ci-retain-db-1:27017\\"},
  #        {_id:1,host:\\"ci-retain-db-2:27017\\"},
  #        {_id:2,host:\\"ci-retain-db-3:27017\\"}
  #     ]})" || true'
  #   restart: "no"

  # ci-retain-worker:
  #   image: python:3.12-slim
  #   depends_on:
  #     ci-retain-rs-init: { condition: service_completed_successfully }
  #   env_file: .env
  #   volumes:
  #     - ./ci_retain_worker/ci_retain_worker.py:/app/ci_retain_worker.py:ro
  #   command: bash -lc "pip install --no-cache-dir requests pymongo && python /app/ci_retain_worker.py"
  #   restart: unless-stopped
  
  sql-adapter:
    image: goncaloferreirauva/gd-wp6-sql-adapter-service
    ports:
      - 8033:8033
    # depends_on:
    #   kpi-service: { condition: service_healthy }
    env_file: .env
    restart: unless-stopped
    command: "uvicorn main:app --host 0.0.0.0 --port 8033 --reload --log-level debug"

### TEMPORARY, just to replace the CIM while we do not have its core.
  cim-service:
    image: python:3.12-slim
    working_dir: /app
    ports:
      - "8012:8012"
    volumes:
      - ./_cim:/app:ro
    command: ["python", "-u", "main.py"]

secrets:
  mongo_keyfile:
    file: ./.secrets/mongo-keyfile
